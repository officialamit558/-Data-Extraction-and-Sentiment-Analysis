{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b1f5770",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\amit7\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\amit7\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#import necessary pacakages\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88cd16ac",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved: C:\\Users\\amit7\\Blackcoffer Assignment\\TitleText\\blackassign0001.txt\n",
      "File saved: C:\\Users\\amit7\\Blackcoffer Assignment\\TitleText\\blackassign0002.txt\n",
      "File saved: C:\\Users\\amit7\\Blackcoffer Assignment\\TitleText\\blackassign0003.txt\n",
      "File saved: C:\\Users\\amit7\\Blackcoffer Assignment\\TitleText\\blackassign0004.txt\n",
      "File saved: C:\\Users\\amit7\\Blackcoffer Assignment\\TitleText\\blackassign0005.txt\n",
      "File saved: C:\\Users\\amit7\\Blackcoffer Assignment\\TitleText\\blackassign0006.txt\n",
      "File saved: C:\\Users\\amit7\\Blackcoffer Assignment\\TitleText\\blackassign0007.txt\n",
      "File saved: C:\\Users\\amit7\\Blackcoffer Assignment\\TitleText\\blackassign0008.txt\n",
      "File saved: C:\\Users\\amit7\\Blackcoffer Assignment\\TitleText\\blackassign0009.txt\n",
      "File saved: C:\\Users\\amit7\\Blackcoffer Assignment\\TitleText\\blackassign0010.txt\n",
      "File saved: C:\\Users\\amit7\\Blackcoffer Assignment\\TitleText\\blackassign0011.txt\n",
      "File saved: C:\\Users\\amit7\\Blackcoffer Assignment\\TitleText\\blackassign0012.txt\n",
      "File saved: C:\\Users\\amit7\\Blackcoffer Assignment\\TitleText\\blackassign0013.txt\n",
      "File saved: C:\\Users\\amit7\\Blackcoffer Assignment\\TitleText\\blackassign0014.txt\n",
      "File saved: C:\\Users\\amit7\\Blackcoffer Assignment\\TitleText\\blackassign0015.txt\n",
      "File saved: C:\\Users\\amit7\\Blackcoffer Assignment\\TitleText\\blackassign0016.txt\n",
      "File saved: C:\\Users\\amit7\\Blackcoffer Assignment\\TitleText\\blackassign0017.txt\n",
      "File saved: C:\\Users\\amit7\\Blackcoffer Assignment\\TitleText\\blackassign0018.txt\n",
      "File saved: C:\\Users\\amit7\\Blackcoffer Assignment\\TitleText\\blackassign0019.txt\n",
      "File saved: C:\\Users\\amit7\\Blackcoffer Assignment\\TitleText\\blackassign0020.txt\n",
      "File saved: C:\\Users\\amit7\\Blackcoffer Assignment\\TitleText\\blackassign0021.txt\n",
      "File saved: C:\\Users\\amit7\\Blackcoffer Assignment\\TitleText\\blackassign0022.txt\n",
      "File saved: C:\\Users\\amit7\\Blackcoffer Assignment\\TitleText\\blackassign0023.txt\n",
      "File saved: C:\\Users\\amit7\\Blackcoffer Assignment\\TitleText\\blackassign0024.txt\n",
      "File saved: C:\\Users\\amit7\\Blackcoffer Assignment\\TitleText\\blackassign0025.txt\n",
      "File saved: C:\\Users\\amit7\\Blackcoffer Assignment\\TitleText\\blackassign0026.txt\n",
      "File saved: C:\\Users\\amit7\\Blackcoffer Assignment\\TitleText\\blackassign0027.txt\n",
      "File saved: C:\\Users\\amit7\\Blackcoffer Assignment\\TitleText\\blackassign0028.txt\n",
      "File saved: C:\\Users\\amit7\\Blackcoffer Assignment\\TitleText\\blackassign0029.txt\n",
      "File saved: C:\\Users\\amit7\\Blackcoffer Assignment\\TitleText\\blackassign0030.txt\n",
      "File saved: C:\\Users\\amit7\\Blackcoffer Assignment\\TitleText\\blackassign0031.txt\n",
      "File saved: C:\\Users\\amit7\\Blackcoffer Assignment\\TitleText\\blackassign0032.txt\n",
      "File saved: C:\\Users\\amit7\\Blackcoffer Assignment\\TitleText\\blackassign0033.txt\n",
      "File saved: C:\\Users\\amit7\\Blackcoffer Assignment\\TitleText\\blackassign0034.txt\n",
      "File saved: C:\\Users\\amit7\\Blackcoffer Assignment\\TitleText\\blackassign0035.txt\n",
      "Error accessing blackassign0036: 404 Client Error: Not Found for url: https://insights.blackcoffer.com/how-neural-networks-can-be-applied-in-various-areas-in-the-future/\n",
      "File saved: C:\\Users\\amit7\\Blackcoffer Assignment\\TitleText\\blackassign0037.txt\n",
      "File saved: C:\\Users\\amit7\\Blackcoffer Assignment\\TitleText\\blackassign0038.txt\n",
      "File saved: C:\\Users\\amit7\\Blackcoffer Assignment\\TitleText\\blackassign0039.txt\n",
      "File saved: C:\\Users\\amit7\\Blackcoffer Assignment\\TitleText\\blackassign0040.txt\n",
      "File saved: C:\\Users\\amit7\\Blackcoffer Assignment\\TitleText\\blackassign0041.txt\n",
      "File saved: C:\\Users\\amit7\\Blackcoffer Assignment\\TitleText\\blackassign0042.txt\n",
      "File saved: C:\\Users\\amit7\\Blackcoffer Assignment\\TitleText\\blackassign0043.txt\n",
      "File saved: C:\\Users\\amit7\\Blackcoffer Assignment\\TitleText\\blackassign0044.txt\n",
      "File saved: C:\\Users\\amit7\\Blackcoffer Assignment\\TitleText\\blackassign0045.txt\n",
      "File saved: C:\\Users\\amit7\\Blackcoffer Assignment\\TitleText\\blackassign0046.txt\n",
      "File saved: C:\\Users\\amit7\\Blackcoffer Assignment\\TitleText\\blackassign0047.txt\n",
      "File saved: C:\\Users\\amit7\\Blackcoffer Assignment\\TitleText\\blackassign0048.txt\n",
      "Error accessing blackassign0049: 404 Client Error: Not Found for url: https://insights.blackcoffer.com/covid-19-environmental-impact-for-the-future/\n",
      "File saved: C:\\Users\\amit7\\Blackcoffer Assignment\\TitleText\\blackassign0050.txt\n",
      "File saved: C:\\Users\\amit7\\Blackcoffer Assignment\\TitleText\\blackassign0051.txt\n",
      "File saved: C:\\Users\\amit7\\Blackcoffer Assignment\\TitleText\\blackassign0052.txt\n",
      "File saved: C:\\Users\\amit7\\Blackcoffer Assignment\\TitleText\\blackassign0053.txt\n",
      "File saved: C:\\Users\\amit7\\Blackcoffer Assignment\\TitleText\\blackassign0054.txt\n",
      "File saved: C:\\Users\\amit7\\Blackcoffer Assignment\\TitleText\\blackassign0055.txt\n",
      "File saved: C:\\Users\\amit7\\Blackcoffer Assignment\\TitleText\\blackassign0056.txt\n",
      "File saved: C:\\Users\\amit7\\Blackcoffer Assignment\\TitleText\\blackassign0057.txt\n",
      "File saved: C:\\Users\\amit7\\Blackcoffer Assignment\\TitleText\\blackassign0058.txt\n",
      "File saved: C:\\Users\\amit7\\Blackcoffer Assignment\\TitleText\\blackassign0059.txt\n",
      "File saved: C:\\Users\\amit7\\Blackcoffer Assignment\\TitleText\\blackassign0060.txt\n",
      "File saved: C:\\Users\\amit7\\Blackcoffer Assignment\\TitleText\\blackassign0061.txt\n",
      "File saved: C:\\Users\\amit7\\Blackcoffer Assignment\\TitleText\\blackassign0062.txt\n",
      "File saved: C:\\Users\\amit7\\Blackcoffer Assignment\\TitleText\\blackassign0063.txt\n",
      "File saved: C:\\Users\\amit7\\Blackcoffer Assignment\\TitleText\\blackassign0064.txt\n",
      "File saved: C:\\Users\\amit7\\Blackcoffer Assignment\\TitleText\\blackassign0065.txt\n",
      "File saved: C:\\Users\\amit7\\Blackcoffer Assignment\\TitleText\\blackassign0066.txt\n",
      "File saved: C:\\Users\\amit7\\Blackcoffer Assignment\\TitleText\\blackassign0067.txt\n",
      "File saved: C:\\Users\\amit7\\Blackcoffer Assignment\\TitleText\\blackassign0068.txt\n",
      "File saved: C:\\Users\\amit7\\Blackcoffer Assignment\\TitleText\\blackassign0069.txt\n",
      "File saved: C:\\Users\\amit7\\Blackcoffer Assignment\\TitleText\\blackassign0070.txt\n",
      "File saved: C:\\Users\\amit7\\Blackcoffer Assignment\\TitleText\\blackassign0071.txt\n",
      "File saved: C:\\Users\\amit7\\Blackcoffer Assignment\\TitleText\\blackassign0072.txt\n",
      "File saved: C:\\Users\\amit7\\Blackcoffer Assignment\\TitleText\\blackassign0073.txt\n",
      "File saved: C:\\Users\\amit7\\Blackcoffer Assignment\\TitleText\\blackassign0074.txt\n",
      "File saved: C:\\Users\\amit7\\Blackcoffer Assignment\\TitleText\\blackassign0075.txt\n",
      "File saved: C:\\Users\\amit7\\Blackcoffer Assignment\\TitleText\\blackassign0076.txt\n",
      "File saved: C:\\Users\\amit7\\Blackcoffer Assignment\\TitleText\\blackassign0077.txt\n",
      "File saved: C:\\Users\\amit7\\Blackcoffer Assignment\\TitleText\\blackassign0078.txt\n",
      "File saved: C:\\Users\\amit7\\Blackcoffer Assignment\\TitleText\\blackassign0079.txt\n",
      "File saved: C:\\Users\\amit7\\Blackcoffer Assignment\\TitleText\\blackassign0080.txt\n",
      "File saved: C:\\Users\\amit7\\Blackcoffer Assignment\\TitleText\\blackassign0081.txt\n",
      "File saved: C:\\Users\\amit7\\Blackcoffer Assignment\\TitleText\\blackassign0082.txt\n",
      "File saved: C:\\Users\\amit7\\Blackcoffer Assignment\\TitleText\\blackassign0083.txt\n",
      "File saved: C:\\Users\\amit7\\Blackcoffer Assignment\\TitleText\\blackassign0084.txt\n",
      "File saved: C:\\Users\\amit7\\Blackcoffer Assignment\\TitleText\\blackassign0085.txt\n",
      "File saved: C:\\Users\\amit7\\Blackcoffer Assignment\\TitleText\\blackassign0086.txt\n",
      "File saved: C:\\Users\\amit7\\Blackcoffer Assignment\\TitleText\\blackassign0087.txt\n",
      "File saved: C:\\Users\\amit7\\Blackcoffer Assignment\\TitleText\\blackassign0088.txt\n",
      "File saved: C:\\Users\\amit7\\Blackcoffer Assignment\\TitleText\\blackassign0089.txt\n",
      "File saved: C:\\Users\\amit7\\Blackcoffer Assignment\\TitleText\\blackassign0090.txt\n",
      "File saved: C:\\Users\\amit7\\Blackcoffer Assignment\\TitleText\\blackassign0091.txt\n",
      "File saved: C:\\Users\\amit7\\Blackcoffer Assignment\\TitleText\\blackassign0092.txt\n",
      "File saved: C:\\Users\\amit7\\Blackcoffer Assignment\\TitleText\\blackassign0093.txt\n",
      "File saved: C:\\Users\\amit7\\Blackcoffer Assignment\\TitleText\\blackassign0094.txt\n",
      "File saved: C:\\Users\\amit7\\Blackcoffer Assignment\\TitleText\\blackassign0095.txt\n",
      "File saved: C:\\Users\\amit7\\Blackcoffer Assignment\\TitleText\\blackassign0096.txt\n",
      "File saved: C:\\Users\\amit7\\Blackcoffer Assignment\\TitleText\\blackassign0097.txt\n",
      "File saved: C:\\Users\\amit7\\Blackcoffer Assignment\\TitleText\\blackassign0098.txt\n",
      "File saved: C:\\Users\\amit7\\Blackcoffer Assignment\\TitleText\\blackassign0099.txt\n",
      "File saved: C:\\Users\\amit7\\Blackcoffer Assignment\\TitleText\\blackassign0100.txt\n"
     ]
    }
   ],
   "source": [
    "# Read the Excel file into the pandas DataFrame\n",
    "df = pd.read_excel('Input.xlsx')\n",
    "\n",
    "# Loop through each row in the DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    url = row['URL']\n",
    "    url_id = row['URL_ID']\n",
    "\n",
    "    # Make a request to the URL\n",
    "    header = {'User-Agent': \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36\"}\n",
    "    try:\n",
    "        response = requests.get(url, headers=header)\n",
    "        response.raise_for_status()  # Raise an exception for HTTP errors\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error accessing {url_id}: {e}\")\n",
    "        continue\n",
    "\n",
    "    # Create a BeautifulSoup object\n",
    "    try:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating BeautifulSoup object for {url_id}: {e}\")\n",
    "        continue\n",
    "\n",
    "    # Find title\n",
    "    try:\n",
    "        title = soup.find('h1').get_text()\n",
    "    except AttributeError:\n",
    "        print(f\"No title found for {url_id}\")\n",
    "        title = \"\"\n",
    "\n",
    "    # Find text\n",
    "    article = \"\"\n",
    "    try:\n",
    "        for p in soup.find_all('p'):\n",
    "            article += p.get_text() + \"\\n\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting text for {url_id}: {e}\")\n",
    "\n",
    "    # Write title and text to the file\n",
    "    file_name = f'C:\\\\Users\\\\amit7\\\\Blackcoffer Assignment\\\\TitleText\\\\{url_id}.txt'\n",
    "    try:\n",
    "        with open(file_name, 'w', encoding='utf-8') as file:\n",
    "            file.write(title + '\\n' + article)\n",
    "        print(f\"File saved: {file_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing to file {file_name}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb6d6808",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de36275c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directories\n",
    "text_dir = r\"C:\\Users\\amit7\\Blackcoffer Assignment\\TitleText\"\n",
    "stopwords_dir = r\"C:\\Users\\amit7\\Blackcoffer Assignment\\StopWords\"\n",
    "sentment_dir = r\"C:\\Users\\amit7\\Blackcoffer Assignment\\MasterDictionary\"\n",
    "\n",
    "# load all stop words from the stopwords directory and store in the set variable\n",
    "stop_words = set()\n",
    "for files in os.listdir(stopwords_dir):\n",
    "  with open(os.path.join(stopwords_dir,files),'r',encoding='ISO-8859-1') as f:\n",
    "    stop_words.update(set(f.read().splitlines()))\n",
    "\n",
    "# load all text files  from the  directory and store in a list(docs)\n",
    "docs = []\n",
    "for text_file in os.listdir(text_dir):\n",
    "  with open(os.path.join(text_dir,text_file),'r' , encoding='ISO-8859-1') as f:\n",
    "    text = f.read()\n",
    "#tokenize the given text file\n",
    "    words = word_tokenize(text)\n",
    "# remove the stop words from the tokens\n",
    "    filtered_text = [word for word in words if word.lower() not in stop_words]\n",
    "# add each filtered tokens of each file into a list\n",
    "    docs.append(filtered_text)\n",
    "\n",
    "\n",
    "\n",
    "# store positive, Negative words from the directory\n",
    "pos=set()\n",
    "neg=set()\n",
    "\n",
    "for files in os.listdir(sentment_dir):\n",
    "  if files =='positive-words.txt':\n",
    "    with open(os.path.join(sentment_dir,files),'r',encoding='ISO-8859-1') as f:\n",
    "      pos.update(f.read().splitlines())\n",
    "  else:\n",
    "    with open(os.path.join(sentment_dir,files),'r',encoding='ISO-8859-1') as f:\n",
    "      neg.update(f.read().splitlines())\n",
    "\n",
    "# now collect the positive  and negative words from each file\n",
    "# calculate the scores from the positive and negative words \n",
    "positive_words = []\n",
    "Negative_words = []\n",
    "positive_score = []\n",
    "negative_score = []\n",
    "polarity_score = []\n",
    "subjectivity_score = []\n",
    "\n",
    "#iterate through the list of docs\n",
    "for i in range(len(docs)):\n",
    "  positive_words.append([word for word in docs[i] if word.lower() in pos])\n",
    "  Negative_words.append([word for word in docs[i] if word.lower() in neg])\n",
    "  positive_score.append(len(positive_words[i]))\n",
    "  negative_score.append(len(Negative_words[i]))\n",
    "  polarity_score.append((positive_score[i] - negative_score[i]) / ((positive_score[i] + negative_score[i]) + 0.000001))\n",
    "  subjectivity_score.append((positive_score[i] + negative_score[i]) / ((len(docs[i])) + 0.000001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e54ab4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Directories\n",
    "text_dir = r\"C:\\Users\\amit7\\Blackcoffer Assignment\\TitleText\"\n",
    "stopwords_dir = r\"C:\\Users\\amit7\\Blackcoffer Assignment\\StopWords\"\n",
    "sentiment_dir = r\"C:\\Users\\amit7\\Blackcoffer Assignment\\MasterDictionary\"\n",
    "\n",
    "\n",
    "# Initialize lists to store calculated metrics\n",
    "avg_sentence_length = []\n",
    "Percentage_of_Complex_words = []\n",
    "Fog_Index = []\n",
    "complex_word_count = []\n",
    "avg_syllable_word_count = []\n",
    "\n",
    "# Load all stop words from the stopwords directory and store in the set variable\n",
    "stop_words_set = set(stopwords.words('english'))\n",
    "\n",
    "def measure(file):\n",
    "    with open(os.path.join(text_dir, file), 'r', encoding='ISO-8859-1') as f:\n",
    "        text = f.read()\n",
    "        # Remove punctuations \n",
    "        text = re.sub(r'[^\\w\\s.]', '', text)\n",
    "        # Split the given text file into sentences\n",
    "        sentences = text.split('.')\n",
    "        # Total number of sentences in a file\n",
    "        num_sentences = len(sentences)\n",
    "        # Total words in the file\n",
    "        words = [word for word in word_tokenize(text) if word.lower() not in stop_words_set]\n",
    "        num_words = len(words)\n",
    "\n",
    "        # Complex words having syllable count greater than 2\n",
    "        # Complex words are words in the text that contain more than two syllables.\n",
    "        complex_words = [word for word in words if len(re.findall(r'[aeiouAEIOU]+', word)) > 2]\n",
    "\n",
    "        # Syllable Count Per Word\n",
    "        # We count the number of syllables in each word of the text by counting the vowels present in each word.\n",
    "        # We also handle some exceptions like words ending with \"es\", \"ed\" by not counting them as a syllable.\n",
    "\n",
    "        syllable_count = sum(len(re.findall(r'[aeiouAEIOU]+', word)) for word in words)\n",
    "        syllable_words = [word for word in words if not word.endswith(('es', 'ed'))]\n",
    "\n",
    "        avg_sentence_len = num_words / num_sentences\n",
    "        avg_syllable_word_count = syllable_count / len(syllable_words)\n",
    "        Percent_Complex_words = len(complex_words) / num_words\n",
    "        Fog_Index = 0.4 * (avg_sentence_len + Percent_Complex_words)\n",
    "\n",
    "        return avg_sentence_len, Percent_Complex_words, Fog_Index, len(complex_words), avg_syllable_word_count\n",
    "\n",
    "# Iterate through each file or doc\n",
    "for file in os.listdir(text_dir):\n",
    "    x, y, z, a, b = measure(file)\n",
    "    avg_sentence_length.append(x)\n",
    "    Percentage_of_Complex_words.append(y)\n",
    "    Fog_Index.append(z)\n",
    "    complex_word_count.append(a)\n",
    "    avg_syllable_word_count.append(b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "df661454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Count and Average Word Length Sum of the total number of characters in each word/Total number of words\n",
    "# We count the total cleaned words present in the text by \n",
    "# removing the stop words (using stopwords class of nltk package).\n",
    "# removing any punctuations like ? ! , . from the word before counting.\n",
    "\n",
    "import os\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Directories\n",
    "text_dir = r\"C:\\Users\\amit7\\Blackcoffer Assignment\\TitleText\"\n",
    "stopwords_dir = r\"C:\\Users\\amit7\\Blackcoffer Assignment\\StopWords\"\n",
    "\n",
    "# Load stopwords from NLTK corpus\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Word Count and Average Word Length: Sum of the total number of characters in each word/Total number of words\n",
    "def cleaned_words(file):\n",
    "    with open(os.path.join(text_dir, file), 'r' , encoding='ISO-8859-1') as f:\n",
    "        text = f.read()\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "        words = [word for word in text.split() if word.lower() not in stop_words]\n",
    "        length = sum(len(word) for word in words)\n",
    "        average_word_length = length / len(words)\n",
    "    return len(words), average_word_length\n",
    "\n",
    "word_count = []\n",
    "average_word_length = []\n",
    "\n",
    "for file in os.listdir(text_dir):\n",
    "    x, y = cleaned_words(file)\n",
    "    word_count.append(x)\n",
    "    average_word_length.append(y)\n",
    "\n",
    "\n",
    "# Count Personal Pronouns mentioned in the text\n",
    "def count_personal_pronouns(file):\n",
    "    with open(os.path.join(text_dir, file), 'r' , encoding='ISO-8859-1') as f:\n",
    "        text = f.read()\n",
    "        personal_pronouns = [\"I\", \"we\", \"my\", \"ours\", \"us\"]\n",
    "        count = 0\n",
    "        for pronoun in personal_pronouns:\n",
    "            count += len(re.findall(r\"\\b\" + pronoun + r\"\\b\", text))  # \\b is used to match word boundaries\n",
    "    return count\n",
    "\n",
    "pp_count = []\n",
    "\n",
    "for file in os.listdir(text_dir):\n",
    "    x = count_personal_pronouns(file)\n",
    "    pp_count.append(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a718be07",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_store = pd.read_excel('Output Data Structure.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7aa7dadf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "output_store.drop(index=[35,48] ,  inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "3db86027",
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = [positive_score,\n",
    "            negative_score,\n",
    "            polarity_score,\n",
    "            subjectivity_score,\n",
    "            avg_sentence_length,\n",
    "            Percentage_of_Complex_words,\n",
    "            Fog_Index,\n",
    "            avg_sentence_length,\n",
    "            complex_word_count,\n",
    "            word_count,\n",
    "            avg_syllable_word_count,\n",
    "            pp_count,\n",
    "            average_word_length]\n",
    "\n",
    "# write the values to the dataframe\n",
    "\n",
    "for i, var in enumerate(variables):\n",
    "  output_store.iloc[:,i+2] = var\n",
    "\n",
    "#now save the dataframe to the disk\n",
    "output_store.to_csv('Output_Data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d1f410db",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.read_csv('Output_Data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "0ff12ab0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "      <th>POSITIVE SCORE</th>\n",
       "      <th>NEGATIVE SCORE</th>\n",
       "      <th>POLARITY SCORE</th>\n",
       "      <th>SUBJECTIVITY SCORE</th>\n",
       "      <th>AVG SENTENCE LENGTH</th>\n",
       "      <th>PERCENTAGE OF COMPLEX WORDS</th>\n",
       "      <th>FOG INDEX</th>\n",
       "      <th>AVG NUMBER OF WORDS PER SENTENCE</th>\n",
       "      <th>COMPLEX WORD COUNT</th>\n",
       "      <th>WORD COUNT</th>\n",
       "      <th>SYLLABLE PER WORD</th>\n",
       "      <th>PERSONAL PRONOUNS</th>\n",
       "      <th>AVG WORD LENGTH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>blackassign0001</td>\n",
       "      <td>https://insights.blackcoffer.com/rising-it-cit...</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>11.645161</td>\n",
       "      <td>0.346260</td>\n",
       "      <td>4.796569</td>\n",
       "      <td>11.645161</td>\n",
       "      <td>125</td>\n",
       "      <td>334</td>\n",
       "      <td>2.311573</td>\n",
       "      <td>2</td>\n",
       "      <td>6.868263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>blackassign0002</td>\n",
       "      <td>https://insights.blackcoffer.com/rising-it-cit...</td>\n",
       "      <td>65</td>\n",
       "      <td>32</td>\n",
       "      <td>0.340206</td>\n",
       "      <td>0.086299</td>\n",
       "      <td>12.797619</td>\n",
       "      <td>0.403721</td>\n",
       "      <td>5.280536</td>\n",
       "      <td>12.797619</td>\n",
       "      <td>434</td>\n",
       "      <td>996</td>\n",
       "      <td>2.624204</td>\n",
       "      <td>4</td>\n",
       "      <td>7.306225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>blackassign0003</td>\n",
       "      <td>https://insights.blackcoffer.com/internet-dema...</td>\n",
       "      <td>46</td>\n",
       "      <td>25</td>\n",
       "      <td>0.295775</td>\n",
       "      <td>0.075372</td>\n",
       "      <td>14.393443</td>\n",
       "      <td>0.505695</td>\n",
       "      <td>5.959655</td>\n",
       "      <td>14.393443</td>\n",
       "      <td>444</td>\n",
       "      <td>819</td>\n",
       "      <td>2.937662</td>\n",
       "      <td>14</td>\n",
       "      <td>7.998779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>blackassign0004</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-cyber...</td>\n",
       "      <td>46</td>\n",
       "      <td>76</td>\n",
       "      <td>-0.245902</td>\n",
       "      <td>0.130761</td>\n",
       "      <td>15.553571</td>\n",
       "      <td>0.473020</td>\n",
       "      <td>6.410636</td>\n",
       "      <td>15.553571</td>\n",
       "      <td>412</td>\n",
       "      <td>817</td>\n",
       "      <td>2.639549</td>\n",
       "      <td>5</td>\n",
       "      <td>7.779682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>blackassign0005</td>\n",
       "      <td>https://insights.blackcoffer.com/ott-platform-...</td>\n",
       "      <td>28</td>\n",
       "      <td>9</td>\n",
       "      <td>0.513513</td>\n",
       "      <td>0.059677</td>\n",
       "      <td>13.466667</td>\n",
       "      <td>0.394389</td>\n",
       "      <td>5.544422</td>\n",
       "      <td>13.466667</td>\n",
       "      <td>239</td>\n",
       "      <td>564</td>\n",
       "      <td>2.481884</td>\n",
       "      <td>7</td>\n",
       "      <td>7.326241</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0           URL_ID  \\\n",
       "0           0  blackassign0001   \n",
       "1           1  blackassign0002   \n",
       "2           2  blackassign0003   \n",
       "3           3  blackassign0004   \n",
       "4           4  blackassign0005   \n",
       "\n",
       "                                                 URL  POSITIVE SCORE  \\\n",
       "0  https://insights.blackcoffer.com/rising-it-cit...              14   \n",
       "1  https://insights.blackcoffer.com/rising-it-cit...              65   \n",
       "2  https://insights.blackcoffer.com/internet-dema...              46   \n",
       "3  https://insights.blackcoffer.com/rise-of-cyber...              46   \n",
       "4  https://insights.blackcoffer.com/ott-platform-...              28   \n",
       "\n",
       "   NEGATIVE SCORE  POLARITY SCORE  SUBJECTIVITY SCORE  AVG SENTENCE LENGTH  \\\n",
       "0               2        0.750000            0.041667            11.645161   \n",
       "1              32        0.340206            0.086299            12.797619   \n",
       "2              25        0.295775            0.075372            14.393443   \n",
       "3              76       -0.245902            0.130761            15.553571   \n",
       "4               9        0.513513            0.059677            13.466667   \n",
       "\n",
       "   PERCENTAGE OF COMPLEX WORDS  FOG INDEX  AVG NUMBER OF WORDS PER SENTENCE  \\\n",
       "0                     0.346260   4.796569                         11.645161   \n",
       "1                     0.403721   5.280536                         12.797619   \n",
       "2                     0.505695   5.959655                         14.393443   \n",
       "3                     0.473020   6.410636                         15.553571   \n",
       "4                     0.394389   5.544422                         13.466667   \n",
       "\n",
       "   COMPLEX WORD COUNT  WORD COUNT  SYLLABLE PER WORD  PERSONAL PRONOUNS  \\\n",
       "0                 125         334           2.311573                  2   \n",
       "1                 434         996           2.624204                  4   \n",
       "2                 444         819           2.937662                 14   \n",
       "3                 412         817           2.639549                  5   \n",
       "4                 239         564           2.481884                  7   \n",
       "\n",
       "   AVG WORD LENGTH  \n",
       "0         6.868263  \n",
       "1         7.306225  \n",
       "2         7.998779  \n",
       "3         7.779682  \n",
       "4         7.326241  "
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4dbf28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27c0fdb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0f5650",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a612f5bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6367d8ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6877caef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31113dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f531a46d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf8b50c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaca1be7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b255b1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
